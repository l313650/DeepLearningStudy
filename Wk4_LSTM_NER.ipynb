{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1084dec-670e-42d8-95ae-95f42b5450cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands of demonstrators have marched throug...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families of soldiers killed in the conflict jo...</td>\n",
       "      <td>['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #                                           Sentence  \\\n",
       "0  Sentence: 1  Thousands of demonstrators have marched throug...   \n",
       "1  Sentence: 2  Families of soldiers killed in the conflict jo...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS', 'IN', 'NNS', 'VBP', 'VBN', 'IN', 'NNP'...   \n",
       "1  ['NNS', 'IN', 'NNS', 'VBN', 'IN', 'DT', 'NN', ...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', '...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(r'./data/TalkFile_ner_2.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e484f1-c7f3-49e8-a315-531941fd7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(x, lower=False):\n",
    "    if lower:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68aa2c4-76f4-4b62-9fb5-ab74fbb94e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>['NNS',  'IN',  'NNS',  'VBP',  'VBN',  'IN', ...</td>\n",
       "      <td>['O',  'O',  'O',  'O',  'O',  'O',  'B-geo', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Families, of, soldiers, killed, in, the, conf...</td>\n",
       "      <td>['NNS',  'IN',  'NNS',  'VBN',  'IN',  'DT',  ...</td>\n",
       "      <td>['O',  'O',  'O',  'O',  'O',  'O',  'O',  'O'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      split_sentence  \\\n",
       "0  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1  [Families, of, soldiers, killed, in, the, conf...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  ['NNS',  'IN',  'NNS',  'VBP',  'VBN',  'IN', ...   \n",
       "1  ['NNS',  'IN',  'NNS',  'VBN',  'IN',  'DT',  ...   \n",
       "\n",
       "                                                 Tag  \n",
       "0  ['O',  'O',  'O',  'O',  'O',  'O',  'B-geo', ...  \n",
       "1  ['O',  'O',  'O',  'O',  'O',  'O',  'O',  'O'...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train['Sentence'].split()\n",
    "\n",
    "def split_sentence(row):\n",
    "    data = {'split_sentence': [], 'POS': [], 'Tag': []}\n",
    "\n",
    "    for column, value in row.items():\n",
    "        if column == 'Sentence':\n",
    "            data['split_sentence'].append(value.split())\n",
    "        if column == 'POS':\n",
    "            data['POS'].append(value[1:-1].split(\",\"))\n",
    "        if column == 'Tag':\n",
    "            data['Tag'].append(value[1:-1].split(\",\"))\n",
    "            \n",
    "    return pd.DataFrame(data)\n",
    "        \n",
    "new_train_df = pd.concat([split_sentence(row) for _, row in train.iterrows()], ignore_index=True)\n",
    "\n",
    "new_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdc8c13-7f93-4851-b5b8-432a3bdff7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                     word_pos_tag\n",
       " 0         (Thousands, 'NNS', 'O')\n",
       " 1               (of,  'IN',  'O')\n",
       " 2   (demonstrators,  'NNS',  'O')\n",
       " 3            (have,  'VBP',  'O')\n",
       " 4         (marched,  'VBN',  'O')\n",
       " 5          (through,  'IN',  'O')\n",
       " 6      (London,  'NNP',  'B-geo')\n",
       " 7               (to,  'TO',  'O')\n",
       " 8          (protest,  'VB',  'O')\n",
       " 9              (the,  'DT',  'O')\n",
       " 10             (war,  'NN',  'O')\n",
       " 11              (in,  'IN',  'O')\n",
       " 12       (Iraq,  'NNP',  'B-geo')\n",
       " 13             (and,  'CC',  'O')\n",
       " 14          (demand,  'VB',  'O')\n",
       " 15             (the,  'DT',  'O')\n",
       " 16      (withdrawal,  'NN',  'O')\n",
       " 17              (of,  'IN',  'O')\n",
       " 18     (British,  'JJ',  'B-gpe')\n",
       " 19         (troops,  'NNS',  'O')\n",
       " 20            (from,  'IN',  'O')\n",
       " 21            (that,  'DT',  'O')\n",
       " 22         (country,  'NN',  'O')\n",
       " 23                (.,  '.',  'O'),\n",
       "                   word_pos_tag\n",
       " 0       (Families, 'NNS', 'O')\n",
       " 1            (of,  'IN',  'O')\n",
       " 2     (soldiers,  'NNS',  'O')\n",
       " 3       (killed,  'VBN',  'O')\n",
       " 4            (in,  'IN',  'O')\n",
       " 5           (the,  'DT',  'O')\n",
       " 6      (conflict,  'NN',  'O')\n",
       " 7       (joined,  'VBD',  'O')\n",
       " 8           (the,  'DT',  'O')\n",
       " 9   (protesters,  'NNS',  'O')\n",
       " 10          (who,  'WP',  'O')\n",
       " 11     (carried,  'VBD',  'O')\n",
       " 12     (banners,  'NNS',  'O')\n",
       " 13         (with,  'IN',  'O')\n",
       " 14         (such,  'JJ',  'O')\n",
       " 15     (slogans,  'NNS',  'O')\n",
       " 16           (as,  'IN',  'O')\n",
       " 17            (\",  '``',  'O')\n",
       " 18    (Bush,  'NNP',  'B-per')\n",
       " 19       (Number,  'NN',  'O')\n",
       " 20          (One,  'CD',  'O')\n",
       " 21    (Terrorist,  'NN',  'O')\n",
       " 22            (\",  '``',  'O')\n",
       " 23          (and,  'CC',  'O')\n",
       " 24            (\",  '``',  'O')\n",
       " 25         (Stop,  'VB',  'O')\n",
       " 26          (the,  'DT',  'O')\n",
       " 27    (Bombings,  'NNS',  'O')\n",
       " 28             (.,  '.',  'O')\n",
       " 29            (\",  '``',  'O')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zip_array(row):\n",
    "    data = {'word_pos_tag': []}\n",
    "    for column, value in row.items():\n",
    "        data['word_pos_tag'] = zip(row['split_sentence'], row['POS'], row['Tag']) \n",
    "    return pd.DataFrame(data)    \n",
    "\n",
    "tokenized_ner_data = [zip_array(row) for _, row in new_train_df.iterrows()]\n",
    "tokenized_ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edc6933b-bdfa-48cc-b65e-66768af87bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the following:\n",
    "# dic_of_words, word_to_id, id_to_word\n",
    "# dic_of_tags, tag_to_id, id_to_tag\n",
    "\n",
    "def create_dic(item_list):\n",
    "    dic = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            for word in item:\n",
    "                if word not in dic:\n",
    "                    dic[word] = 1\n",
    "                else:\n",
    "                    dic[word] += 1\n",
    "    return dic\n",
    "\n",
    "def create_mapping(dic):\n",
    "    sorted_items = sorted(dic.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "    \n",
    "def word_mapping(sentences_df):\n",
    "    words = [ [ [ word[0].lower() for _, word in w.items()] for _, w in s.items()] for s in sentences_df]\n",
    "    dic = create_dic(words)\n",
    "    dic['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dic)\n",
    "    return dic, word_to_id, id_to_word\n",
    "\n",
    "def tag_mapping(sentences_df):\n",
    "    tags = [ [ [ word[-1] for _, word in w.items()] for _, w in s.items()] for s in sentences_df]\n",
    "    dic = create_dic(tags)\n",
    "    tag_to_id, id_to_tag = create_mapping(dic)\n",
    "    return dic, tag_to_id, id_to_tag\n",
    "    \n",
    "    \n",
    "\n",
    "dic_words, word_to_id, id_to_word = word_mapping(tokenized_ner_data)\n",
    "dic_tags, tag_to_id, id_to_tag = tag_mapping(tokenized_ner_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "039e8ca4-a0d0-4e41-bbec-1d049125fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences\n",
    "def prepare_dataset(sentences, word_to_id, tag_to_id):\n",
    "    data = []\n",
    "\n",
    "    for s in sentences:\n",
    "        for _, str_words in s.items():\n",
    "            strs = [w[0] for w in str_words]\n",
    "            words = [word_to_id[w[0] if w[0] in word_to_id else '<UNK>'] for w in str_words]\n",
    "            tags = [tag_to_id[w[-1]] for w in str_words]\n",
    "\n",
    "            data.append({\n",
    "                'str_words': strs,\n",
    "                'words': words,\n",
    "                'tags': tags\n",
    "            })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(tokenized_ner_data, word_to_id, tag_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac0d422-2b23-4f66-a064-134f2c164f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 pretrained embeddings.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "all_word_embeds = {}\n",
    "for i, line in enumerate(codecs.open(\"./data/glove.6B.100d.txt\", 'r', 'utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == 100 + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "#Intializing Word Embedding Matrix\n",
    "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), 100))\n",
    "\n",
    "for w in word_to_id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0775d42b-5fec-4257-9775-2df2b718996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_file = './data/mapping.pkl'\n",
    "\n",
    "# with open(mapping_file, 'wb') as f:\n",
    "#     mappings = {\n",
    "#         'word_to_id': word_to_id,\n",
    "#         'tag_to_id': tag_to_id\n",
    "#     }\n",
    "#     cPickle.dump(mappings, f)\n",
    "\n",
    "# print('word_to_id: ', len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73df854-8e46-498f-b83e-97c8fbc9c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "\n",
    "class NamedEntityRecog(nn.Module):\n",
    "    def __init__(self, vocab_size, word_embed_dim, word_hidden_dim, tag_num, dropout, pretrain_embed=None):\n",
    "        super(NamedEntityRecog, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.input_dim = word_embed_dim\n",
    "\n",
    "        self.embeds = nn.Embedding(vocab_size, word_embed_dim, padding_idx=0)\n",
    "        self.embeds.weight.data.copy_(torch.from_numpy(pretrain_embed))\n",
    "\n",
    "        self.lstm = nn.LSTM(self.input_dim, word_hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(word_hidden_dim * 2, tag_num)\n",
    "\n",
    "    def random_embedding(self, vocab_size, embedding_dim):\n",
    "        pretrain_emb = np.empty([vocab_size, embedding_dim])\n",
    "        scale = np.sqrt(3.0 / embedding_dim)\n",
    "        for index in range(1, vocab_size):\n",
    "            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedding_dim])\n",
    "        return pretrain_emb\n",
    "\n",
    "    def neg_log_likelihood_loss(self, word_inputs, word_seq_lengths, batch_label, mask):\n",
    "        batch_size = word_inputs.size(0)\n",
    "        seq_len = word_inputs.size(1)\n",
    "        word_embeding = self.embeds(word_inputs)\n",
    "        word_list = [word_embeding]\n",
    "        word_embeding = torch.cat(word_list, 2)\n",
    "        word_represents = self.drop(word_embeding)\n",
    "        packed_words = pack_padded_sequence(word_represents, word_seq_lengths, True)\n",
    "        hidden = None\n",
    "        lstm_out, hidden = self.lstm(packed_words, hidden)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out)\n",
    "        lstm_out = lstm_out.transpose(0, 1)\n",
    "        feature_out = self.drop(lstm_out)\n",
    "\n",
    "        feature_out = self.hidden2tag(feature_out)\n",
    "\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "        feature_out = feature_out.contiguous().view(batch_size * seq_len, -1)\n",
    "        total_loss = loss_function(feature_out, batch_label.contiguous().view(batch_size * seq_len))\n",
    "        return total_loss\n",
    "\n",
    "    def forward(self, word_inputs, word_seq_lengths, batch_label, mask):\n",
    "        batch_size = word_inputs.size(0)\n",
    "        seq_len = word_inputs.size(1)\n",
    "        word_embeding = self.embeds(word_inputs)\n",
    "        word_list = [word_embeding]\n",
    "        word_embeding = torch.cat(word_list, 2)\n",
    "        word_represents = self.drop(word_embeding)\n",
    "        packed_words = pack_padded_sequence(word_represents, word_seq_lengths, True)\n",
    "        hidden = None\n",
    "        lstm_out, hidden = self.lstm(packed_words, hidden)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out)\n",
    "        lstm_out = lstm_out.transpose(0, 1)\n",
    "        feature_out = self.drop(lstm_out)\n",
    "\n",
    "        feature_out = self.hidden2tag(feature_out)\n",
    "\n",
    "        feature_out = feature_out.contiguous().view(batch_size * seq_len, -1)\n",
    "        _, tag_seq = torch.max(feature_out, 1)\n",
    "        tag_seq = tag_seq.view(batch_size, seq_len)\n",
    "        tag_seq = mask.long() * tag_seq\n",
    "        return tag_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1fc17-f2eb-48dd-a679-1dbcb3303fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NamedEntityRecog(dic_words.size(), 100, 2, tag_to_id.size(), 0.5, word_embeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6a3b5d-de33-4e1b-89c4-95c3c6aa6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n",
    "\n",
    "train_begin = time.time()\n",
    "print('train begin', '-' * 50)\n",
    "print()\n",
    "print()\n",
    "\n",
    "writer = SummaryWriter('log')\n",
    "batch_num = -1\n",
    "best_f1 = -1\n",
    "early_stop = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    epoch_begin = time.time()\n",
    "    print('train {}/{} epoch'.format(epoch + 1, args.epochs))\n",
    "    optimizer = lr_decay(optimizer, epoch, 0.05, args.lr)\n",
    "    batch_num = train_model(train_dataloader, model, optimizer, batch_num, writer, use_gpu)\n",
    "    new_f1 = evaluate(dev_dataloader, model, word_vocab, label_vocab, pred_file, score_file, eval_script, use_gpu)\n",
    "    print('f1 is {} at {}th epoch on dev set'.format(new_f1, epoch + 1))\n",
    "    if new_f1 > best_f1:\n",
    "        best_f1 = new_f1\n",
    "        print('new best f1 on dev set:', best_f1)\n",
    "        early_stop = 0\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "    else:\n",
    "        early_stop += 1\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    cost_time = epoch_end - epoch_begin\n",
    "    print('train {}th epoch cost {}m {}s'.format(epoch + 1, int(cost_time / 60), int(cost_time % 60)))\n",
    "    print()\n",
    "\n",
    "    if early_stop > args.patience:\n",
    "        print('early stop')\n",
    "        break\n",
    "\n",
    "train_end = time.time()\n",
    "train_cost = train_end - train_begin\n",
    "hour = int(train_cost / 3600)\n",
    "min = int((train_cost % 3600) / 60)\n",
    "second = int(train_cost % 3600 % 60)\n",
    "print()\n",
    "print()\n",
    "print('train end', '-' * 50)\n",
    "print('train total cost {}h {}m {}s'.format(hour, min, second))\n",
    "print('-' * 50)\n",
    "\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "test_acc = evaluate(test_dataloader, model, word_vocab, label_vocab, pred_file, score_file, eval_script, use_gpu)\n",
    "print('test acc on test set:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ab2d7-b7b0-46ae-8504-7ccded4817cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
